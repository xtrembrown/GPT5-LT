# GPT5-LT
Purpose: Circumvent hallucinations (producing false, misleading, or nonsensical information as if it were factual or true) prevalent in Generative Pre-trained Transformer Models. 
#  
Usage V1.0: Attach "GPT5_Less-Thinking_V1.0" to each conversation under "Files", reccomended with GPT5-Thinking model and disabling "Reference saved memories"; found in Settings -> Personalization -> Memory -> Reference Saved Memories. Use this to allow you to think, and produce sources on requested information, not to produce an answer. 

    - For each prompt please enter "[Expected Outcome]: One to four sentences of what you expect in a response, followed by what you do not want". Removing the quotation marks, and in your words.
#  
Exception: Developed for ChatGPT 5 thinking model, though acceptable answers have been found on ChatGPT's flagship free model.  

Note: Model V0.1 is considered much stable than V0.2. The updated model V1.0 is considered better than both, minor tweaks will be updated the following weeks, though appoears very stable. 

Reccomended: Watch this video by PyCon USA, from the keynote speaker _Simon Willison_; for a explanation on how Generative Pre-trained Transformer Models operate, specifically Large Language Models (LLM). 
